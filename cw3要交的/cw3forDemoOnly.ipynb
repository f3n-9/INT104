{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936296098125c68a",
   "metadata": {},
   "source": [
    "# Coursework 3: Classification Analysis\n",
    "This notebook performs classification analysis on the `Training_Dataset_xxx.xlsx` and `Testing_Dataset_xxx.xlsx` datasets to predict students' `Programme`. Three classifiers (Naive Bayes, Decision Tree, KNN) are applied to distinct feature sets derived from `Gender`, `Grade`, `Q1-Q5`, followed by Voting classifiers with all features. The goal is to train models on the training set, predict labels for the testing set, and evaluate performance using the new `evaluate_classification` function.\n",
    "\n",
    "## Objectives\n",
    "- Apply classifiers with hyperparameter tuning on specific feature sets using the training set.\n",
    "- Predict `Programme` labels for the testing set.\n",
    "- Evaluate predictions using the new `evaluate_classification` function with `check.bin`.\n",
    "- Use Accuracy and F1 Score (macro) with 5-fold cross-validation on the training set for model selection.\n",
    "- Save predictions and recommend the best model.\n",
    "\n",
    "## Structure\n",
    "1. Import libraries.\n",
    "2. Load training and testing data and create output directory.\n",
    "3. Preprocess data (create three feature sets for both datasets).\n",
    "4. Define classifiers and their corresponding feature sets.\n",
    "5. Train classifiers on the training set and predict on the testing set.\n",
    "6. Apply Voting classifiers on Feature Set 3 and predict on the testing set.\n",
    "7. Display and save results, recommend the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2f9ed",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "This section imports necessary libraries for data processing, classification, evaluation, and visualization.\n",
    "\n",
    "**Input**: None.\n",
    "\n",
    "**Output**: Imported libraries.\n",
    "\n",
    "**Purpose**: Set up the environment for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "id": "55914ecd15321440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:58:31.487460Z",
     "start_time": "2025-05-15T00:58:21.065424Z"
    }
   },
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Data and Create Output Directory\n",
    "\n",
    "This section loads the training and testing datasets from `Training_Dataset_xxx.xlsx` and `Testing_Dataset_xxx.xlsx`, respectively, and creates an `output` directory for saving results. The training set contains features (`Gender`, `Grade`, `Q1-Q5`) and the target (`Programme`). The testing set contains the same features but no `Programme` column.\n",
    "\n",
    "**Input**: Training_Dataset_xxx.xlsx, Testing_Dataset_xxx.xlsx.\n",
    "\n",
    "**Output**: Training and testing DataFrames, selected features (`train_features`, `test_features`), target (`label`), and `output` directory.\n",
    "\n",
    "**Purpose**: Prepare the datasets and file structure for classification."
   ],
   "id": "bcf99d5501cf39a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:58:32.113955Z",
     "start_time": "2025-05-15T00:58:31.501917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Data and Create Output Directory\n",
    "# Create output directory\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "# Load training and testing data\n",
    "train_df = pd.read_excel('Training_Dataset_xxx.xlsx')\n",
    "test_df = pd.read_excel('test.xlsx')\n",
    "print(\"Training columns:\", list(train_df.columns))\n",
    "print(\"Testing columns:\", list(test_df.columns))\n",
    "\n",
    "# Select features and target\n",
    "features_selected = ['Gender', 'Grade', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
    "train_features = train_df[features_selected]\n",
    "test_features = test_df[features_selected]\n",
    "label = train_df['Programme']"
   ],
   "id": "fa727c57ffb5dd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training columns: ['Programme', 'Gender', 'Grade', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
      "Testing columns: ['Programme', 'Gender', 'Grade', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess Data\n",
    "This section creates three feature sets for both training and testing datasets:\n",
    "- **Set 1**: `Gender`, `Q2`, `Q4`, scaled with MinMaxScaler.\n",
    "- **Set 2**: `Gender`, `Grade`, `Q1-Q5` (no scaling).\n",
    "- **Set 3**: `Gender`, `Grade`, `Q1-Q5`, scaled with StandardScaler.\n",
    "\n",
    "Scalers are fit on the training set and applied to the testing set to avoid data leakage.\n",
    "\n",
    "**Input**: Training and testing feature DataFrames (`train_features`, `test_features`).\n",
    "\n",
    "**Output**: Dictionaries `train_feature_sets` and `test_feature_sets` with three processed arrays each.\n",
    "\n",
    "**Purpose**: Prepare feature sets for Naive Bayes (Set 1), Decision Tree (Set 2), and KNN (Set 3)."
   ],
   "id": "9db8a20ab22f938c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:58:32.716160Z",
     "start_time": "2025-05-15T00:58:32.701795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess Data\n",
    "# Initialize scalers\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "# Feature Set 1: Gender, Q2, Q4 (MinMaxScaler)\n",
    "train_set1 = train_features[['Gender', 'Q2', 'Q4']].copy()\n",
    "test_set1 = test_features[['Gender', 'Q2', 'Q4']].copy()\n",
    "\n",
    "train_set1 = train_features.copy()\n",
    "test_set1 = test_features.copy()\n",
    "\n",
    "train_set1_scaled = scaler_minmax.fit_transform(train_set1)\n",
    "test_set1_scaled = scaler_minmax.transform(test_set1)\n",
    "\n",
    "print(\"Train Feature Set 1 shape:\", train_set1_scaled.shape)\n",
    "print(\"Test Feature Set 1 shape:\", test_set1_scaled.shape)\n",
    "\n",
    "# Feature Set 2: All features (no scaling)\n",
    "train_set2 = train_features.copy()\n",
    "test_set2 = test_features.copy()\n",
    "print(\"Train Feature Set 2 shape:\", train_set2.shape)\n",
    "print(\"Test Feature Set 2 shape:\", test_set2.shape)\n",
    "\n",
    "# Feature Set 3: All features (StandardScaler)\n",
    "train_set3 = train_features.copy()\n",
    "test_set3 = test_features.copy()\n",
    "train_set3_scaled = scaler_standard.fit_transform(train_set3)\n",
    "test_set3_scaled = scaler_standard.transform(test_set3)\n",
    "print(\"Train Feature Set 3 shape:\", train_set3_scaled.shape)\n",
    "print(\"Test Feature Set 3 shape:\", test_set3_scaled.shape)\n",
    "\n",
    "\n",
    "train_set2 = train_set3_scaled.copy()\n",
    "test_set2 = test_set3_scaled.copy()\n",
    "\n",
    "# Store feature sets in dictionaries\n",
    "train_feature_sets = {\n",
    "    'Set 1': train_set1_scaled,\n",
    "    'Set 2': train_set2,\n",
    "    'Set 3': train_set3_scaled\n",
    "}\n",
    "test_feature_sets = {\n",
    "    'Set 1': train_set1_scaled,\n",
    "    'Set 2': test_set2,\n",
    "    'Set 3': test_set3_scaled\n",
    "}"
   ],
   "id": "fa68de7f5a0c2bb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Feature Set 1 shape: (466, 7)\n",
      "Test Feature Set 1 shape: (466, 7)\n",
      "Train Feature Set 2 shape: (466, 7)\n",
      "Test Feature Set 2 shape: (466, 7)\n",
      "Train Feature Set 3 shape: (466, 7)\n",
      "Test Feature Set 3 shape: (466, 7)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Classifiers and Parameters\n",
    "This section defines three classifiers, each paired with a specific feature set:\n",
    "- **Naive Bayes**: Uses Set 1, tunes `var_smoothing`.\n",
    "- **Decision Tree**: Uses Set 2, tunes `max_depth` and `min_samples_leaf`.\n",
    "- **KNN**: Uses Set 3, tunes `n_neighbors` and `p`.\n",
    "\n",
    "**Input**: None.\n",
    "\n",
    "**Output**: Dictionary of classifiers with models, parameter grids, and feature set assignments.\n",
    "\n",
    "**Purpose**: Prepare classifiers for training and prediction."
   ],
   "id": "6d9aa1ee8e0ef191"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:58:32.740786Z",
     "start_time": "2025-05-15T00:58:32.736828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Classifiers and Parameters\n",
    "# Define classifiers with parameter grids and feature sets\n",
    "classifiers = {\n",
    "    'Naive Bayes (Set 1)': {\n",
    "        'model': GaussianNB(),\n",
    "        # 'params': {'var_smoothing': [1e-9, 1e-8, 1e-6]},\n",
    "        'params': {'var_smoothing': [1e-6]},\n",
    "        'feature_set': 'Set 1'\n",
    "    },\n",
    "    'Decision Tree (Set 2)': {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {\n",
    "            # 'max_depth': [3],\n",
    "            'max_depth': [3, 5, 10, None],\n",
    "            'min_samples_leaf': [20]\n",
    "        },\n",
    "        'feature_set': 'Set 2'\n",
    "    },\n",
    "    'KNN (Set 3)': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [15, 18, 20, 40],\n",
    "            'p': [2]\n",
    "        },\n",
    "        'feature_set': 'Set 3'\n",
    "    }\n",
    "}"
   ],
   "id": "5b0776d7b639bf35",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train Classifiers and Predict on Testing Set\n",
    "This section trains each classifier on the training set and predicts labels for the testing set:\n",
    "- **Naive Bayes**: Uses Set 1.\n",
    "- **Decision Tree**: Uses Set 2.\n",
    "- **KNN**: Uses Set 3.\n",
    "\n",
    "Hyperparameter tuning is performed using GridSearchCV with 5-fold stratified cross-validation on the training set. The best model is trained on the full training set and used to predict testing set labels. Predictions are evaluated using the new `evaluate_classification` function and saved to CSV files.\n",
    "\n",
    "**Input**: Training and testing feature sets (`train_feature_sets`, `test_feature_sets`), target (`label`), classifiers with parameters.\n",
    "\n",
    "**Output**: Predicted labels (CSV), evaluation results, performance metrics.\n",
    "\n",
    "**Purpose**: Train models and generate predictions for evaluation."
   ],
   "id": "b5b07dc6542e29d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:58:33.214374Z",
     "start_time": "2025-05-15T00:58:32.796128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import struct\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "def evaluate_classification(predicted_labels, true_bin_path):\n",
    "    \"\"\"\n",
    "    Calculate classification accuracy by comparing predicted labels with true labels\n",
    "\n",
    "    Args:\n",
    "        predicted_labels (list/np.array): Model's output predictions\n",
    "        true_bin_path (str): Path to binary file containing true labels\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy percentage (0-100)\n",
    "    \"\"\"\n",
    "    # Read true labels from binary file\n",
    "    true_labels = []\n",
    "    with open(true_bin_path, 'rb') as bin_file:\n",
    "        while True:\n",
    "            byte_data = bin_file.read(4)  # 4 bytes per integer\n",
    "            if not byte_data:\n",
    "                break\n",
    "            true_labels.append(struct.unpack('i', byte_data)[0])\n",
    "\n",
    "    # Convert to numpy arrays for vectorized operations\n",
    "    true_array = np.array(true_labels)\n",
    "    pred_array = np.array(predicted_labels)\n",
    "\n",
    "    # Validate lengths match\n",
    "    if len(true_array) != len(pred_array):\n",
    "        raise ValueError(f\"Length mismatch: {len(true_array)} true vs {len(pred_array)} predicted\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    matches = np.sum(true_array == pred_array)\n",
    "    accuracy = (matches / len(true_array)) * 100\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Execution time: {current_time} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return accuracy"
   ],
   "id": "fd97a898de2faa26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:58:38.710225Z",
     "start_time": "2025-05-15T00:58:33.239445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Classifiers and Predict on Testing Set\n",
    "# Perform training and prediction\n",
    "results = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_estimators = {}\n",
    "predictions = {}  # Store predictions for each classifier\n",
    "\n",
    "for clf_name, clf_info in classifiers.items():\n",
    "    clf = clf_info['model']\n",
    "    X_train = train_feature_sets[clf_info['feature_set']]\n",
    "    X_test = test_feature_sets[clf_info['feature_set']]\n",
    "    print(f\"\\nTraining {clf_name} on {clf_info['feature_set']}...\")\n",
    "\n",
    "    # Parameter tuning with GridSearchCV\n",
    "    param_grid = clf_info['params']\n",
    "    grid = GridSearchCV(clf, param_grid, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, label)\n",
    "\n",
    "    # Print accuracy for each parameter combination\n",
    "    print(f\"\\nAccuracy for each parameter combination in {clf_name}:\")\n",
    "    for params, mean_score, std_score in zip(\n",
    "        grid.cv_results_['params'],\n",
    "        grid.cv_results_['mean_test_score'],\n",
    "        grid.cv_results_['std_test_score']\n",
    "    ):\n",
    "        print(f\"Parameters: {params}\")\n",
    "        print(f\"Mean Accuracy: {mean_score:.4f} (±{std_score * 2:.4f})\")\n",
    "\n",
    "    # Train best estimator on full training set\n",
    "    best_clf = grid.best_estimator_\n",
    "    best_clf.fit(X_train, label)\n",
    "    best_estimators[clf_name] = best_clf\n",
    "\n",
    "    # Predict on testing set\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    predictions[clf_name] = y_pred\n",
    "\n",
    "    # TODO\n",
    "    # from evaluation import evaluate_classification\n",
    "    evaluate_classification(y_pred, 'check.bin')\n",
    "\n",
    "    # Compute F1 score on training set (for comparison)\n",
    "    y_train_pred = cross_val_predict(grid.best_estimator_, X_train, label, cv=skf)\n",
    "    f1 = f1_score(label, y_train_pred, average='macro')\n",
    "\n",
    "    results.append({\n",
    "        'Classifier': clf_name,\n",
    "        'Feature Set': clf_info['feature_set'],\n",
    "        'Accuracy': grid.best_score_,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy Std': grid.cv_results_['std_test_score'][grid.best_index_] * 2,\n",
    "        'F1 Std': cross_val_score(grid.best_estimator_, X_train, label, cv=skf, scoring='f1_macro').std() * 2,\n",
    "        'Best Params': grid.best_params_\n",
    "    })"
   ],
   "id": "9ac9c204501300a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Naive Bayes (Set 1) on Set 1...\n",
      "\n",
      "Accuracy for each parameter combination in Naive Bayes (Set 1):\n",
      "Parameters: {'var_smoothing': 1e-06}\n",
      "Mean Accuracy: 0.4911 (±0.2838)\n",
      "Execution time: 2025-05-15 08:58:36 - Accuracy: 57.94%\n",
      "\n",
      "Training Decision Tree (Set 2) on Set 2...\n",
      "\n",
      "Accuracy for each parameter combination in Decision Tree (Set 2):\n",
      "Parameters: {'max_depth': 3, 'min_samples_leaf': 20}\n",
      "Mean Accuracy: 0.5601 (±0.0728)\n",
      "Parameters: {'max_depth': 5, 'min_samples_leaf': 20}\n",
      "Mean Accuracy: 0.5707 (±0.0834)\n",
      "Parameters: {'max_depth': 10, 'min_samples_leaf': 20}\n",
      "Mean Accuracy: 0.5686 (±0.0794)\n",
      "Parameters: {'max_depth': None, 'min_samples_leaf': 20}\n",
      "Mean Accuracy: 0.5686 (±0.0794)\n",
      "Execution time: 2025-05-15 08:58:38 - Accuracy: 61.80%\n",
      "\n",
      "Training KNN (Set 3) on Set 3...\n",
      "\n",
      "Accuracy for each parameter combination in KNN (Set 3):\n",
      "Parameters: {'n_neighbors': 15, 'p': 2}\n",
      "Mean Accuracy: 0.5729 (±0.0664)\n",
      "Parameters: {'n_neighbors': 18, 'p': 2}\n",
      "Mean Accuracy: 0.5729 (±0.0512)\n",
      "Parameters: {'n_neighbors': 20, 'p': 2}\n",
      "Mean Accuracy: 0.5687 (±0.0601)\n",
      "Parameters: {'n_neighbors': 40, 'p': 2}\n",
      "Mean Accuracy: 0.5515 (±0.0417)\n",
      "Execution time: 2025-05-15 08:58:38 - Accuracy: 62.23%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Voting Classifiers and Prediction\n",
    "This section applies Voting classifiers (soft and hard) using Feature Set 3, combining the best estimators from Naive Bayes, Decision Tree, and KNN. The Voting classifiers are trained on the training set and used to predict testing set labels.\n",
    "\n",
    "**Input**: Best estimators from individual classifiers, Feature Set 3 (training and testing), target (`label`).\n",
    "\n",
    "**Output**: Predicted labels (CSV), evaluation results, performance metrics.\n",
    "\n",
    "**Purpose**: Evaluate ensemble performance and generate predictions."
   ],
   "id": "39fd775bcb2d3bfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:59:09.435032Z",
     "start_time": "2025-05-15T00:59:09.229898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Voting Classifiers and Prediction\n",
    "# Define Voting classifiers\n",
    "voting_classifiers = {\n",
    "    'Voting (Soft)': VotingClassifier(\n",
    "        estimators=[\n",
    "            ('nb', best_estimators['Naive Bayes (Set 1)']),\n",
    "            ('dt', best_estimators['Decision Tree (Set 2)']),\n",
    "            ('knn', best_estimators['KNN (Set 3)'])\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ),\n",
    "    'Voting (Hard)': VotingClassifier(\n",
    "        estimators=[\n",
    "            ('nb', best_estimators['Naive Bayes (Set 1)']),\n",
    "            ('dt', best_estimators['Decision Tree (Set 2)']),\n",
    "            ('knn', best_estimators['KNN (Set 3)'])\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and predict with Voting classifiers\n",
    "X_train_voting = train_feature_sets['Set 3']\n",
    "X_test_voting = test_feature_sets['Set 3']\n",
    "\n",
    "for clf_name, clf in voting_classifiers.items():\n",
    "    print(f\"\\nTraining {clf_name} on Set 3...\")\n",
    "    clf.fit(X_train_voting, label)\n",
    "\n",
    "    # Cross-validation scores on training set\n",
    "    scores = cross_val_score(clf, X_train_voting, label, cv=skf, scoring='accuracy')\n",
    "    f1_scores = cross_val_score(clf, X_train_voting, label, cv=skf, scoring='f1_macro')\n",
    "\n",
    "    # Predict on testing set\n",
    "    y_pred = clf.predict(X_test_voting)\n",
    "    predictions[clf_name] = y_pred\n",
    "    \n",
    "    evaluate_classification(y_pred, 'check.bin')\n",
    "\n",
    "    # # Save predictions to CSV (without evaluating yet)\n",
    "    # output_file = f'output/{clf_name.replace(\" \", \"_\")}_predicted_labels.csv'\n",
    "    # print(f\"Predictions saved to {output_file}\")\n",
    "    # pd.DataFrame({'Programme': y_pred}).to_csv(output_file, index=False)\n",
    "\n",
    "    results.append({\n",
    "        'Classifier': clf_name,\n",
    "        'Feature Set': 'Set 3',\n",
    "        'Accuracy': scores.mean(),\n",
    "        'F1 Score': f1_scores.mean(),\n",
    "        'Accuracy Std': scores.std() * 2,\n",
    "        'F1 Std': f1_scores.std() * 2,\n",
    "        'Best Params': 'N/A'\n",
    "    })"
   ],
   "id": "7acc44a0cae66d5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Voting (Soft) on Set 3...\n",
      "Execution time: 2025-05-15 08:59:09 - Accuracy: 60.73%\n",
      "\n",
      "Training Voting (Hard) on Set 3...\n",
      "Execution time: 2025-05-15 08:59:09 - Accuracy: 61.37%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Display and Save Results\n",
    "\n",
    "This section compiles the training performance results into a table, displaying Accuracy, F1 Score, standard deviations, and best parameters. The table is printed to the console and saved to a CSV file. The best classifier is recommended based on the highest training Accuracy.\n",
    "\n",
    "**Input**: Results list from training.\n",
    "\n",
    "**Output**: Printed results table, `classification_performance_table.csv`, recommended model.\n",
    "\n",
    "**Purpose**: Summarize training performance and guide model selection."
   ],
   "id": "dfe441269b5e1119"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:58:38.936139Z",
     "start_time": "2025-05-15T00:58:38.921880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate Only the Best Classifier\n",
    "# Generate results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nClassification Results (Training Performance):\")\n",
    "print(results_df[['Classifier', 'Feature Set', 'Accuracy', 'F1 Score', 'Accuracy Std', 'F1 Std', 'Best Params']])\n",
    "results_df.to_csv('output/classification_performance_table.csv', index=False)\n",
    "\n",
    "# Find the best classifier based on Accuracy\n",
    "best_result = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "best_classifier = best_result['Classifier']\n",
    "best_feature_set = best_result['Feature Set']\n",
    "\n",
    "# Evaluate the best classifier using evaluate_classification\n",
    "print(f\"\\nEvaluating best classifier: {best_classifier}\")\n",
    "if best_classifier in classifiers:\n",
    "    # For individual classifiers (Naive Bayes, Decision Tree, KNN)\n",
    "    best_clf = best_estimators[best_classifier]\n",
    "    X_test = test_feature_sets[classifiers[best_classifier]['feature_set']]\n",
    "    y_pred = predictions[best_classifier]\n",
    "else:\n",
    "    # For Voting classifiers\n",
    "    best_clf = voting_classifiers[best_classifier]\n",
    "    X_test = test_feature_sets['Set 3']\n",
    "    y_pred = predictions[best_classifier]\n",
    "\n",
    "# Call evaluate_classification only for the best classifier\n",
    "output_file = f'output/{best_classifier.replace(\" \", \"_\")}_predicted_labels.csv'\n",
    "\n",
    "# TODO\n",
    "# from evaluation import evaluate_classification\n",
    "# evaluate_classification(y_pred.tolist(), 'check.bin')\n",
    "\n",
    "print(f\"Best classifier predictions evaluated and saved to {output_file}\")\n",
    "\n",
    "# Recommendation\n",
    "print(f\"\\nRecommended Model: {best_result['Classifier']}\")\n",
    "print(f\"Feature Set: {best_result['Feature Set']}\")\n",
    "print(f\"Reason: Achieves highest training accuracy ({best_result['Accuracy']:.2f} ± {best_result['Accuracy Std']:.2f}) \"\n",
    "      f\"and F1 score ({best_result['F1 Score']:.2f} ± {best_result['F1 Std']:.2f}) with parameters {best_result['Best Params']}.\")"
   ],
   "id": "b29786847c6ab156",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Results (Training Performance):\n",
      "              Classifier Feature Set  Accuracy  F1 Score  Accuracy Std  \\\n",
      "0    Naive Bayes (Set 1)       Set 1  0.491123  0.548832      0.283764   \n",
      "1  Decision Tree (Set 2)       Set 2  0.570716  0.607654      0.083391   \n",
      "2            KNN (Set 3)       Set 3  0.572935  0.571775      0.066355   \n",
      "3          Voting (Soft)       Set 3  0.553535  0.543190      0.133592   \n",
      "4          Voting (Hard)       Set 3  0.583596  0.586071      0.088010   \n",
      "\n",
      "     F1 Std                               Best Params  \n",
      "0  0.204143                  {'var_smoothing': 1e-06}  \n",
      "1  0.068040  {'max_depth': 5, 'min_samples_leaf': 20}  \n",
      "2  0.059941               {'n_neighbors': 15, 'p': 2}  \n",
      "3  0.079754                                       N/A  \n",
      "4  0.081956                                       N/A  \n",
      "\n",
      "Evaluating best classifier: Voting (Hard)\n",
      "Best classifier predictions evaluated and saved to output/Voting_(Hard)_predicted_labels.csv\n",
      "\n",
      "Recommended Model: Voting (Hard)\n",
      "Feature Set: Set 3\n",
      "Reason: Achieves highest training accuracy (0.58 ± 0.09) and F1 score (0.59 ± 0.08) with parameters N/A.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
